---
title: "Prediction Assignment"
author: "Iain Dalrymple"
date: "Tuesday, February 23, 2016"
output: html_document
---
 
# Introduction
The goal of this data analysis is to predict the manner in which participants in a study did exercise, which is to say how well they did it. The training data set can be downloaded [here](https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv) and the small test set of 20 samples to be evaluated can be downloaded [here](https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv).  

The data for this project comes from this source [http://groupware.les.inf.puc-rio.br/har](http://groupware.les.inf.puc-rio.br/har), where further information can be found. 

These data are from wearable accelerometers and contain information from these wearable accelerometers under different intensities of exercise. 

The modelling will seek to predict the variable "classe" which is a factor with 5 levels. The exercise completed was sets of 10 repetitions of unilateral dumbbell bicep curls.  

- Class A was for exercise exactly according to the specification
- Class B was for exercise throwing the elbows to the front
- Class c was for exercise lifting the dumbbell only halfway
- Class D was for exercise lowering the dumbbell only halfway
- Class E was for exercise throwing the hips to the front

```{r, echo=FALSE, warning=FALSE, message=FALSE, cache=TRUE}
# set WD and load packages
setwd("~/Coursera/PML")
library(caret)
set.seed(1111)

# read raw data and set #DIV/0 to NA
train<-read.csv("pml-training.csv", na.strings=c("NA","#DIV/0!", ""), row.names=1)
quiz.test<-read.csv("pml-testing.csv", na.strings=c("NA","#DIV/0!", ""), row.names=1)
dt1<-dim(train)

# clean the datasets- remove data with too many NAs and near 0 variance
nilvar<-nearZeroVar(train, saveMetrics=TRUE)
train<-train[,nilvar$nzv==FALSE]
nNa<-sapply(train, function(x) sum(is.na(x)))
remcol<-NULL
for (i in 1:length(nNa))
    {
    if (nNa[i]/nrow(train)>0.9)
        {
        remcol[i]<-TRUE
        }
    else
        {
        remcol[i]<-FALSE
        }
    }
train<-train[,remcol==FALSE]
train<-train[,6:ncol(train)]

```

## Reproducability 
Reproducability has been achieved by setting the seed to 1111 for all code. In order to reproduce the results, this must be maintained in subsequent data interrogations. 

# Dataset cleaning and Preparation. 
A initial review of the training and testing data sets show that there are `r dt1[2]` columns of data and `r dt1[1]` rows. There are a significant number of non-numeric data points in the data set, including "NA", "#DIV/0!" and empty data cells. These have all been converted to NA data.  
Columns with more than 90% of the data as NA have been removed and columns with a near zero variance have been removed by the nearZeroVar() function. Several rows of irrelevant data at the beginning of the data set have been removed also. The resultant final data set has `r ncol(train)` columns and the same number of rows as initially.  

```{r, echo=FALSE, warning=FALSE, message=FALSE, cache=TRUE}
t.set<-createDataPartition(y=train$classe, p=0.7, list=FALSE)
train.set<-train[t.set,]
test.set<-train[-t.set,]
```

The cleaned training data set has been subdivided into a model training set and a model testing set using the createDataPartition() function, with 70% of the data going to the training set grouped by the classe variable. The resultant model training set has `r nrow(train.set)` rows of data while the testing set has `r nrow(test.set)` rows. This will allow for cross validation and the calculation for out of sample error. 

# Model Assesment
Three different modelling methods will be evaluated for this data review. We will compare the efficiency of random forests, naive Bayes and stochastic gradient boosting. Each will be plied using repeated cross validation using K folds with 5 folds and 3 repeats.  

```{r, echo=FALSE, warning=FALSE, message=FALSE, cache=TRUE}
tr_cont<-trainControl(method="repeatedcv", number=5, repeats = 3)
modelrf<-train(classe ~. , data= train.set, trControl=tr_cont, method="rf")
modelnb<-train(classe ~. , data= train.set, trControl=tr_cont, method="nb")
modelgbm<-train(classe ~. , data= train.set, 
                trControl=tr_cont, method="gbm", verbose=FALSE)

predismodelrf<-predict(modelrf, train.set)
predismodelnb<-predict(modelnb, train.set)
predismodelgbm<-predict(modelgbm, train.set)
predosmodelrf<-predict(modelrf, test.set)
predosmodelnb<-predict(modelnb, test.set)
predosmodelgbm<-predict(modelgbm, test.set)

errortab<-matrix(data=NA, nrow=3, ncol=2)
errortab[1,1]<-round(100-confusionMatrix(train.set$classe,predismodelrf)$overall[1]*100,1)
errortab[1,2]<-round(100-confusionMatrix(test.set$classe,predosmodelrf)$overall[1]*100,1)
errortab[2,1]<-round(100-confusionMatrix(train.set$classe,predismodelnb)$overall[1]*100,1)
errortab[2,2]<-round(100-confusionMatrix(test.set$classe,predosmodelnb)$overall[1]*100,1)
errortab[3,1]<-round(100-confusionMatrix(train.set$classe,predismodelgbm)$overall[1]*100,1)
errortab[3,2]<-round(100-confusionMatrix(test.set$classe,predosmodelgbm)$overall[1]*100,1)

colnames(errortab)<-c("In Sample Error","Out of Sample Error")
rownames(errortab)<-c("Random Forests","Naive Bayes","GBM")

```

From these data, the confusion matrices have been calculated for the training subset and testing subset for each of the models. 

### Confusion matrix for Random Forests model training subset
```{r, echo=FALSE, warning=FALSE, message=FALSE}
confusionMatrix(train.set$classe, predismodelrf)
```

### Confusion matrix for Random Forests model testing subset
```{r, echo=FALSE, warning=FALSE, message=FALSE}
confusionMatrix(test.set$classe, predosmodelrf)
```

### Confusion matrix for Naive Bayes model training subset
```{r, echo=FALSE, warning=FALSE, message=FALSE}
confusionMatrix(train.set$classe, predismodelnb)
```

### Confusion matrix for Naive Bayes model testing subset
```{r, echo=FALSE, warning=FALSE, message=FALSE}
confusionMatrix(test.set$classe, predosmodelnb)
```

### Confusion matrix for Sochastic Gradient Boosting model training subset
```{r, echo=FALSE, warning=FALSE, message=FALSE}
confusionMatrix(train.set$classe, predismodelgbm)
```

### Confusion matrix for Sochastic Gradient Boosting model testing subset
```{r, echo=FALSE, warning=FALSE, message=FALSE}
confusionMatrix(test.set$classe, predosmodelgbm)
```

The accuracy of these models is summarized in the table below. Note that the efficiency of the Random Forests approach shows that the out of bag error estimated in the confusion table for the training subset is a very good match for the out of sample error, showing that it is a truly unbiased estimator of the model and that cross validation was not necessary. Below this is a graph of the 20 most important variables in the model. 

```{r, echo=FALSE, warning=FALSE, message=FALSE, cache=TRUE}
errortab
plot(varImp(modelrf), top = 20)
```

It is clear that there is a clear winner in terms of model prediction accuracy, with Random Forests approach exceeding the other two models in terms of both in sample accuracy and out of sample accuracy. 

It is the Random Forests approach that will be used to evaluate the  test data set of 20 samples. This is unsurprising as a Random Forest approach is well suited to large data set with numerous inputs with unknown interactions. 

# Test Set Predictions
As stated in the introduction, a small test set of 20 samples was also provided to be evaluated by the final Random Forest model. As a check, the predictions were also made from the final Stochastic Gradient Boosting model which was almost as accurate, but not quite. The predictions of both models are shown below. Fortunately, the predictions from both models are identical, giving good confidence in the results. 

```{r, echo=FALSE, warning=FALSE, message=FALSE, results='asis'}
quizrfpred<-predict(modelrf, newdata=quiz.test)
quizgbmpred<-predict(modelgbm, newdata=quiz.test)
cat("Predictions from the final Random Forest model")
cat('\n\n')
quizrfpred
cat('\n\n')
cat("Predictions from the final Stochastic Gradient Boosting model")
cat('\n\n')
quizgbmpred
```
